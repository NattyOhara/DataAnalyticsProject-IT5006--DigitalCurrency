{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 - Time Series \n",
    "\n",
    "## Learning Objectives\n",
    "+ Understanding time-series concepts.\n",
    "+ Seasonality and seasonal decomposition\n",
    "+ Stationarity\n",
    "    + Why need stationarity\n",
    "    + How to check\n",
    "    + How to make a time-series stationary\n",
    "+ Understanding concept of differencing\n",
    "+ Time series forecasting\n",
    "\n",
    "The contents of this tutorial are based on \"Python for Data Analysis\" by McKinney, \"Python Machine Learning Case Studies\" by Danish Haroon, [tutorial](https://www.machinelearningplus.com/time-series/time-series-analysis-python/) on time series analysis, [tutorial](https://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python/) on ARIMA model, [tutorial](https://sigmundojr.medium.com/seasonality-in-python-additive-or-multiplicative-model-d4b9cf1f48a7) on seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Consider a financial dataset originally obtained from Yahoo! Finance containing end-of-day prices for a few stocks and the S&P 500 index (the SPX symbol) \n",
    "\n",
    "\n",
    "Time series analysis is usually done with the objective of forecasting the long-term trend over time as\n",
    "per the problem’s underlying hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "close_px_all = pd.read_csv('stock_px_2.csv', parse_dates=True, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_px = close_px_all[['AAPL', 'MSFT', 'XOM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot a moving average of the series. ```rolling()``` is similar in behavior to ```groupby```, but instead of grouping it creates an object that enables grouping over a x-day sliding window. So here we create the 250-day moving window average of Apple’s stock price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Concepts\n",
    "Any time series may be split into the following components: Base Level + Trend + Seasonality + Error\n",
    "\n",
    "A **trend** is observed when there is an increasing or decreasing slope observed in the time series. Whereas **seasonality** is observed when there is a distinct repeated pattern observed between regular intervals due to seasonal factors. It could be because of the month of the year, the day of the month, weekdays or even time of the day.\n",
    "\n",
    "However, It is not mandatory that all time series must have a trend and/or seasonality. A time series may not have a distinct trend but have a seasonality. The opposite can also be true.\n",
    "\n",
    "So, a time series may be imagined as a combination of the trend, seasonality and the error terms.\n",
    "\n",
    "Another aspect to consider is the **cyclic** behaviour. It happens when the rise and fall pattern in the series does not happen in fixed calendar-based intervals. Care should be taken to not confuse ‘cyclic’ effect with ‘seasonal’ effect. So, How to diffentiate between a ‘cyclic’ vs ‘seasonal’ pattern? If the patterns are not of fixed calendar based frequencies, then it is cyclic. Because, unlike the seasonality, cyclic effects are typically influenced by the business and other socio-economic factors.\n",
    "\n",
    "\n",
    "# Additive and Multiplicative Models\n",
    "Depending on the nature of the trend and seasonality, a time series can be modeled as an additive or multiplicative, wherein, each observation in the series can be expressed as either a sum or a product of the components:\n",
    "\n",
    "Additive time series:\n",
    "Value = Base Level + Trend + Seasonality + Error\n",
    "\n",
    "Multiplicative Time Series:\n",
    "Value = Base Level x Trend x Seasonality x Error\n",
    "\n",
    "You can do a classical decomposition of a time series by considering the series as an additive or multiplicative combination of the base level, trend, seasonal index and the residual.\n",
    "\n",
    "Note that the additive model does not vary in frequency and amplitude over time. The multiplicative model does, in this second model, the behavior acts as an increasing funnel (which may be decreasing) \n",
    "\n",
    "\n",
    "![Seasonality](https://miro.medium.com/max/564/1*LdeXlKrgNkFUjOhnO4Zzaw.jpeg)\n",
    "\n",
    "We use multiplicative models when the magnitude of the seasonal pattern in the data depends on the magnitude of the data. On other hand, in the additive model, the magnitude of seasonality does not change in relation to time.\n",
    "\n",
    "The [```seasonal_decompose```](https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html) in ```statsmodels``` implements this conveniently.\n",
    "Setting ```extrapolate_trend='freq'``` takes care of any missing values in the trend and residuals at the beginning of the series.\n",
    "\n",
    "Let us assume that the period is 1 month, i.e. 20 trading days per month. You might want to check with other candidate parameter values too (5 trading days in a week and 253 in a year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_apl = close_px['AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_add = seasonal_decompose(st_apl, model='additive', period=20, extrapolate_trend='freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mul = seasonal_decompose(st_apl, model='multiplicative', period=20, extrapolate_trend='freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the residuals of the additive decomposition closely, it has some pattern left over. The multiplicative decomposition, however, looks quite random which is good. So ideally, multiplicative decomposition should be preferred for this particular series.\n",
    "\n",
    "The numerical output of the trend, seasonal and residual components are stored in the result_mul output itself. Let’s extract them and put it in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you check, the product of seas, trend and resid columns should exactly equal to the actual_values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationary Time Series\n",
    "Most of the time series models work on the assumption that data is stationary. Moreover,\n",
    "contrary theories related to stationary time series are easier to implement than non-\n",
    "stationary time series theories. A time series object is stationary if it has the following\n",
    "properties:\n",
    "+ No trend exists\n",
    "+ Mean remains constant over time\n",
    "+ Variance remains constant over time\n",
    "+ No autocorrelation exists. Autocorrelation is the correlation between the series at current time with a lagged version of itself.\n",
    "\n",
    "It is possible to make nearly any time series stationary by applying a suitable transformation. Most statistical forecasting methods are designed to work on a stationary time series. The first step in the forecasting process is typically to do some transformation to convert a non-stationary series to stationary.\n",
    "\n",
    "Autoregressive forecasting models are essentially linear regression models that utilize the lag(s) of the series itself as predictors.We know that linear regression works best if the predictors (X variables) are not correlated against each other. So, stationarizing the series solves this problem since it removes any persistent autocorrelation, thereby making the predictors(lags of the series) in the forecasting models nearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests to Determine if time series is stationary\n",
    "\n",
    "1. Augmented Dickey-Fuller Test (other tests exist too, such as KPSS test, PP test)\n",
    "2. Exploratory Data Analysis\n",
    "    + Visual method of finding if distribution is stationary\n",
    "    + Split the series into 2 or more contiguous parts and computing the summary statistics like the mean, variance and the autocorrelation. If the stats are quite different, then the series is not likely to be stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented Dickey-Fuller Test\n",
    "\n",
    "Null hypothesis is the time series possesses a unit root and is non-stationary. A linear stochastic process has a unit root if 1 is a root of the process's characteristic equation. Such a process is non-stationary but does not always have a trend. \\[Source: [Wikipedia](https://en.wikipedia.org/wiki/Unit_root)\\] So, if the P-Value in ADH test is less than the significance level (0.05), you reject the null hypothesis.\n",
    "\n",
    "The KPSS test, on the other hand, is used to test for trend stationarity. The null hypothesis and the P-Value interpretation is just the opposite of ADH test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = adfuller(result_mul.resid.dropna(), autolag='AIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of ADF test: The null hypothesis is rejected as its probability of occurance (p-value) is extremely low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = adfuller(result_add.resid.dropna(), autolag='AIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even in case of additive decomposition, ADF suggests to null hypothesis. However can we conclude that the series is stationary? No, as the heteroscedasticity in the residual is apparent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods of Making a Time-Series Stationary\n",
    "1. Applying transformations - such as log transformation\n",
    "2. Estimating trend and removing it from the original series\n",
    "3. Differencing\n",
    "4. Decomposition\n",
    "\n",
    "If the volume of the seasonal effect is directly proportional to the mean, then the seasonal effect is said to be multiplicative, and a logarithmic transformation is needed to make it additive again. This is because ARIMA expects data that is either not seasonal or has the seasonal component removed, e.g. seasonally adjusted via methods such as seasonal differencing.\n",
    "\n",
    "If we assume our series to be additive, then it is not stationary. Let us try to make it stationary! \n",
    "\n",
    "First let us apply log transformation to the series.\n",
    "\n",
    "## Applying log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_apl_log = np.log(st_apl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differencing\n",
    "Apart from seasonal decomposition, another common approach to make the series stationary, or remove the trend line, is differencing. \n",
    "\n",
    "Let $y$ be the given time-series, and $y_t$ is its value at time $t$. In differencing, a trend-free time-series $z$ is computed from $y$. This depends on the nature of trendline in $y$. If it is linear, then $z$ is obtained by $$z_t = y_t-y_{t-1},$$\n",
    "which is called as first order differencing. If the trendline is quadratic, then $z$ is obtained by doing differencing of differenced $y$,\n",
    "$$z_t = (y_t-y_{t-1}) - (y_{t-1}-y_{t-2}),$$\n",
    "which is called as second order differencing. Similarly, one can remove higher ordered trendlines as well by further differencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Model\n",
    "In ARIMA, the given time-series $y$ is modelled using 3 components:\n",
    "+ *Auto Regressive* (AR)\n",
    "+ *Integrated* (I)\n",
    "+ *Moving Average* (MA)\n",
    "\n",
    "The AR and MA are essentially linear regression models: the target variable is the detrended (stationary) time-series $z$ obtained after differencing. The role of *Integration* (I) will become clear in a while.  \n",
    "In AR, $z_t$ is modeled in terms of its `p` *lagged* values\n",
    "$$z_t = c + \\beta_1 z_{t-1} + \\cdots + \\beta_p z_{t-p} + \\epsilon_t,$$\n",
    "where $\\epsilon_t$ is the random error at time $t$ and $c$ is the constant term of regression.  \n",
    "Whereas in MA, $z_t$ modeled in terms of `q` lagged error terms:\n",
    "$$z_t = c + \\theta_1 \\epsilon_{t-1} + \\cdots + \\theta_q \\epsilon_{t-q} + \\epsilon_t.$$\n",
    "And together in ARMA, $z_t$ is modeled as \n",
    "$$z_t = c + \\beta_1 z_{t-1} + \\cdots + \\beta_p z_{t-p} + \\theta_1 \\epsilon_{t-1} + \\cdots + \\theta_q \\epsilon_{t-q} + \\epsilon_t.$$\n",
    "\n",
    "<font color=blue>Why are we doing linear regression on stationary time-series of $y$ instead of directly on $y$?</font>\n",
    "\n",
    "But How do we recover $y$ from $z$? This is done in the reverse way of how we found $z$ from $y$, which is differencing. And reverse of differencing (discrete differentiation) is integration (I). You see, in case of a first order differencing, one can recover $y_t$ as \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_t &= (y_t - y_{t-1}) + (y_{t-1} - y_{t-2}) + \\cdots + (y_{2} - y_1) + (y_1 - y_0) + y_0 \\\\\n",
    "    &= z_t + z_{t-1} + \\cdots + z_2 + z_1 + y_0 \\\\\n",
    "    &= \\sum_{s=1}^{t} z_{s} + y_0.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is a discrete version of integration $y(t) = \\int_{s=0}^t z(s) ds + y(0)$. Similarly, one can derive integration for `d`th order differencing as well.\n",
    "\n",
    "So, ARIMA model is specified by the orders of its respective components `(p,d,q)`.  \n",
    "But how to find the orders? You could just try some different combinations of terms and see what works best.\n",
    "It can also be done by systematically by examining the correlations in the time series -- Autocorrelations and Partial Autocorrelations -- by plotting them. However, for the tutorial, we not go into understanding this method, and pick up a combination of terms and run the forecasting.\n",
    "\n",
    "So, simply put, ‘p’ is the order of the ‘Auto Regressive’ (AR) term. It refers to the number of lags of Y to be used as predictors. Let us use 4 lag as predictor for our model - a random choice for the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(10, 8))\n",
    "\n",
    "# Original Series\n",
    "axes[0, 0].plot(st_apl_log); plt.title('Original')\n",
    "axes[0, 0].set_title('Original Series')\n",
    "plot_acf(st_apl_log, ax=axes[0, 1])\n",
    "\n",
    "# 1st Differencing\n",
    "axes[1, 0].plot(st_apl_log.diff())\n",
    "axes[1, 0].set_title('1st Order Differencing')\n",
    "plot_acf(st_apl_log.diff().dropna(), ax=axes[1, 1])\n",
    "\n",
    "# 2nd Differencing\n",
    "axes[2, 0].plot(st_apl_log.diff().diff())\n",
    "axes[2, 0].set_title('2nd Order Differencing')\n",
    "plot_acf(st_apl_log.diff().diff().dropna(), ax=axes[2, 1])\n",
    "\n",
    "plt.tight_layout(pad=2.0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [```ARIMA```](https://www.statsmodels.org/devel/generated/statsmodels.tsa.arima_model.ARIMA.html) model available in ```statsmodel```. We can use the ```fit```  function which returns [ARIMAResults](https://www.statsmodels.org/devel/generated/statsmodels.tsa.arima_model.ARIMAResults.html#statsmodels.tsa.arima_model.ARIMAResults) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary reveals a lot of information. The table in the middle is the coefficients table where the values under ‘coef’ are the weights of the respective terms. We typically want a model such that the terms are significant (the p-values column) - which happens to be in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s plot the residuals to ensure there are no patterns (that is, look for constant mean and variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual errors seem fine with near zero mean and uniform variance. Let’s plot the actuals against the fitted values using ```plot_predict()```. When you set ```dynamic=False``` the in-sample lagged values are used for prediction. That is, the model gets trained up until the previous value to make the next prediction. This can make the fitted forecast and actuals look artificially good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Time cross-validation\n",
    "\n",
    "For a real-validation, let us do a Out-of-Time cross-validation. In Out-of-Time cross-validation, you take few steps back in time and forecast into the future to as many steps you took back. Then you compare the forecast against the actuals.\n",
    "\n",
    "To do out-of-time cross-validation, you need to create the training and testing dataset by splitting the time series into 2 contiguous parts in approximately 75:25 ratio or a reasonable proportion based on time frequency of series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_apl_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = st_apl_log[:1660]\n",
    "test = st_apl_log[1660:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the chart, the ARIMA(4,1,4) model seems to give a directionally correct forecast. And the actual observed values lie within the 95% confidence band. That seems fine.\n",
    "\n",
    "Ideally, you should go back multiple points in time, like, go back 1, 2, 3 and 4 quarters and see how your forecasts are performing at various points in the timeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for evaluation\n",
    "\n",
    "The commonly used accuracy metrics to judge forecasts are:\n",
    "\n",
    "1. Mean Absolute Percentage Error (MAPE)\n",
    "2. Mean Error (ME)\n",
    "3. Mean Absolute Error (MAE)\n",
    "4. Mean Percentage Error (MPE)\n",
    "5. Root Mean Squared Error (RMSE)\n",
    "6. Lag 1 Autocorrelation of Error (ACF1)\n",
    "7. Correlation between the Actual and the Forecast (corr)\n",
    "\n",
    "Typically, if you are comparing forecasts of two different series, the MAPE and Correlation can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def forecast_accuracy(forecast, actual):\n",
    "    mape = np.mean(np.abs(forecast - actual)/np.abs(actual))  # MAPE - Mean absolute percentage error\n",
    "    me = np.mean(forecast - actual)             # ME - mean error\n",
    "    mae = np.mean(np.abs(forecast - actual))    # MAE - mean absolute error\n",
    "    mpe = np.mean((forecast - actual)/actual)   # MPE - mean percentage error\n",
    "    rmse = np.mean((forecast - actual)**2)**.5  # RMSE\n",
    "    corr = np.corrcoef(forecast, actual)[0,1]   # corr\n",
    "    acf1 = acf(fc-test)[1]                      # ACF1\n",
    "    return({'mape':mape, 'me':me, 'mae': mae, \n",
    "            'mpe': mpe, 'rmse':rmse, 'acf1':acf1, \n",
    "            'corr':corr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Exercise (Optional):\n",
    "1. Compute a DataFrame consisting of the yearly correlations of daily returns (computed from percent changes) with SPX using the original csv file. For this, first create a function that computes the pairwise correlation of each column with the 'SPX' column. Then compute percent change on close_px using ```pct_change``` function available in pandas.\n",
    "2. Lookup the ```resample``` function in pandas. Resampling refers to the process of converting a time series from one frequency to another. Aggregating higher frequency data to lower frequency is called downsampling, while converting lower frequency to higher frequency is called upsampling. Not all resampling falls into either of these categories; for example, converting W-WED (weekly on Wednesday) to W-FRI is neither upsampling nor downsampling. ```resample``` has similar API to ```groupby``` - you group the data and then call an aggregate function. How can you resample the data to a business day frequency? \n",
    "3. What would you use for forecasting in case of multiplicative decomposition? \n",
    "4. The current forecasts are in log transformed series. Transform them back to the original series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
