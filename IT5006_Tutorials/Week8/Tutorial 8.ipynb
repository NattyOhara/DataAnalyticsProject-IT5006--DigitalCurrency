{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 - Clustering \n",
    "\n",
    "## Learning Objectives\n",
    "+ Understanding High-dimensional data\n",
    "+ t-SNE on data to understand and reduce dimensions of data\n",
    "+ Hierarchical clustering\n",
    "+ K-Means clustering\n",
    "    + Difference with hierarchical clustering\n",
    "    + Methods of finding number of clusters:\n",
    "        + Using silhoutte analysis\n",
    "        + Using elbow method\n",
    "\n",
    "The contents of this tutorial are based on the chapter 10 of \"An Introduction to Statistical Learning\" by James G. et. al., python [notebook](https://nbviewer.jupyter.org/github/JWarmenhoven/ISL-python/blob/master/Notebooks/Chapter%2010.ipynb#Lab-3:-NCI60-Data-Example) on same chapter, sklearn [tutorial](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py) on k-means, sklearn [tutorial](https://scikit-learn.org/stable/modules/clustering.html) on clustering, sklearn [tutorial](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html) on agglomerative clustering, and introduction to dimensionality reduction [tutorial](https://www.datacamp.com/community/tutorials/introduction-t-sne)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the NCI-160 dataset\n",
    "\n",
    "NCI60 is cancer cell line microarray data, which consists of 6,830 gene expression measurements on 64 cancer cell lines. So, our data is 64x6830, implying that the n<\\<p. In case of such high-dimensional data, methods such as regression do not remain applicable. Dimensionality reduction is also useful for such data, before applying different ML models. \n",
    "\n",
    "Gene data is usually always a high-dimensionality data, and is useful in precision medicine - i.e. to recommend personalized treatment to patients. A common approach for such personalized treatment is categorizing the patients into various sub-types. For such sub-typing, clustering is usually performed and can be used to identify patients who are similar - to analyze their treatments and decide the further course of treatment for the patient under consideration.  \n",
    "\n",
    "In our data, though we have cancer type label for each cell-line, in real-life data, we typically do not have this for all the patients, making the clustering quite important. Usually clustering methods are then analyzed on various datasets, and validation from the domain - such as the cancer types in our case are used to identify the best technique to be used. \n",
    "\n",
    "Our data has gene expression measurement which is usually achieved by quantifying levels of the gene product, which is often a protein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('nci.data.csv', skiprows=1, header=None).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.iloc[0]                                 # Set new column names\n",
    "df.drop(0,inplace=True)                                 # Drop duplicated row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = pd.read_csv('nci.label.txt', names=['type'], dtype=\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs.type = labs.type.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "Let us first try to find out whether or not the observations cluster into distinct types of cancer. \n",
    "\n",
    "Here we have an option to standardize the variables to have mean zero and standard deviation one. However, this step is optional and should be performed only if we want each gene to be on the same scale. We could reasonably argue that it is better not to scale genes. However, it is a choice to make as a data analyst, and also evaluate experimentally.\n",
    "\n",
    "## Agglomerative Clustering\n",
    "This is a bottom-up clustering, and is built starting from the leaves and combining clusters up to the trunk. Each dendogram will represent each of our 64 samples. As we move up the tree, some leaves begin to fuse into branches. These correspond to\n",
    "observations that are similar to each other. As we move higher up the tree, branches themselves fuse, either with leaves or other branches. The earlier (lower in the tree) fusions occur, the more similar the groups of observations are to each other. Thus, the height of the fusion depicts how different the observations are.\n",
    "\n",
    "The [```AgglomerativeClustering```](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html) class in sklearn implements four different types of linkages. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion.\n",
    "+ ‘ward’ minimizes the variance of the clusters being merged.\n",
    "+ ‘average’ uses the average of the distances of each observation of the two sets.\n",
    "+ ‘complete’ or ‘maximum’ linkage uses the maximum distances between all observations of the two sets.\n",
    "+ ‘single’ uses the minimum of the distances between all observations of the two sets.\n",
    "\n",
    "The default is ward. Usually complete linkage is used for Gene data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us write a function to plot dendogram. We use the [```cluster.hierarchy.dendogram```](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram). The [```cluster.hierarchy```]() is available in scipy which has functions that cut hierarchical clusterings into flat clusterings or find the roots of the forest formed by a cut by providing the flat cluster ids of each observation. We will see usage of both these API - the sklearn and the scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the scipy API and also scale the gene data and see the clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the choice of linkage\n",
    "certainly does affect the results obtained. Typically, single linkage will tend\n",
    "to yield trailing clusters: very large clusters onto which individual observations attach one-by-one. On the other hand, complete and average linkage\n",
    "tend to yield more balanced, attractive clusters. For this reason, complete\n",
    "and average linkage are generally preferred to single linkage. \n",
    "\n",
    "In the complete linkage, we can set the threshold to get four clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means for same number of clusters\n",
    "We can perform K-Means with same number of clusters and check the clusters that are formed. \n",
    "\n",
    "In K Means clustering, since we start with random choice of clusters, the results produced by running the algorithm multiple times might differ. While results are reproducible in Hierarchical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that though clustering has been done in same number of clusters - the observations per cluster is different and the clusters themselves must also be different. You can do visualizations along with the cancer types to actually see the difference between the clusters using both algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding k \n",
    "Unlike the hierarchical clustering, the k-means clustering has the problem of specifying the number of clusters (k). How do we find this? We have two popular methods for this approach:\n",
    "+ Silhoutte Method\n",
    "+ Elbow Method\n",
    "\n",
    "## Silhoutte method\n",
    "Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of \\[-1, 1\\].\n",
    "\n",
    "Silhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.\n",
    "\n",
    "### Performing dimensionality reduction - PCA\n",
    "Sometimes performing clustering on the first few principal component score vectors can give better results than performing clustering on the full data. In this situation, we might view the principal component step as one of denoising the data. You can ofcourse perform the clustering without the PCA, however, on this specific data, the PCA helps in finding more meaningful clusters using K-means. So, let us first perform PCA on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(random_state=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_plot = pd.DataFrame(pca.fit_transform(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now once we have this principal components, the 64 dimensional data can be used to perform silhoutte analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhoutte_plot(n_clusters):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # The silhouette coefficient can range from -1, 1, but ranges from -0.1 to 1 for our data\n",
    "    ax.set_xlim([-0.1, 1])\n",
    "    \n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax.set_ylim([0, len(df2_plot) + (n_clusters + 1) * 10])\n",
    "\n",
    "    clusterer = KMeans(n_clusters=n_clusters, n_init=50, random_state=rng)\n",
    "    cluster_labels = clusterer.fit_predict(df2_plot)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(df2_plot, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(df2_plot, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "        \n",
    "    ax.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "\n",
    "    plt.title((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Method\n",
    "This is another method used for selecting the number of clusters. We fit the model with range of K values. If the line chart resembles an arm, then the “elbow” (the point of inflection on the curve) is a good indication that the underlying model fits best at that point. \n",
    "\n",
    "### Performing dimensionality reduction - tSNE\n",
    "t-Distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. It is extensively applied in image processing, NLP, genomic data and speech processing. In simple terms, t-SNE minimizes the divergence between two distributions: a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding. This is a particularly useful technique for visualizing high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=rng)\n",
    "df3 = pd.DataFrame(tsne.fit_transform(df), columns=['tsne1', 'tsne2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal number of clusters - we plot the sum of squared distances of samples to their closest cluster center, available as ```inertia_``` attribute of the K-Means class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "k_list = range(1, 20)\n",
    "for k in k_list:\n",
    "    km = KMeans(n_clusters=k, random_state=rng)\n",
    "    km.fit(df3)\n",
    "    sse.append(km.inertia_)\n",
    "    \n",
    "tsne_results_scale = pd.DataFrame({'Cluster': range(1,20), 'SSE': sse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE often fails to preserve the global geometry of the data. This means that the relative position of clusters on the t-SNE plot is almost arbitrary and depends on random initialisation more than on anything else. While this may not be a problem in some situations, scRNA-seq data sets often exhibit biologically meaningful hierarchical structure, e.g. encompass several very different cell classes, each further divided into various types. Typical t-SNE plots do not capture such global structure, yielding a suboptimal and potentially misleading visualisation. So, t-SNE may not always yeild best dimensionality reduction for genetic data, however, it has shown very good results for high-dimensional data in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would we analyze which clustering is most representative for this type of data?\n",
    "Usually the domain knowledge of the cancer types and which algorithm has given us good clusters which are most representative is evaluated. Another approach used for gene data is to evaluate the different patient characteristics, such as Age, co-morbidities, etc. to find if the clusters are actually identifying such patterns. Also, using multiple datasets is usually required in such unsupervised tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Exercise (Optional)\n",
    "1. Can you plot the t-SNE and PCA with the cancer type labels and try to reason which dimensionality reduction might be more appropriate for our data? \n",
    "2. How does clustering vary when using hierarchical clustering on few principal components? Are these results different from the ones we obtain?\n",
    "3. In this tutorial, we explored data where n<<p,what happens when n>>p? What might be the bottlenecks for doing k-means? The concept of batching might seem useful in this case. You can use [```make_blobs```](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) to generate synthetic data for clustering and explore using the [```MiniBatchKMeans```](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans) algorithm. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
